\documentclass{article}
\usepackage[margin=1in]{geometry} % smaller margins

\title{CS273A Project Report}
\author{The Vectors Anonymous Support Group\\Kyle Benson, Eugenia Gabrielova}
\date{December 2013}
\begin{document}
\maketitle

%TODO who did what?

\section{Introduction}

The Knowledge Discovery and Data Mining (KDD) Cup in 2004 involved a problem for learning which of two classes a particle generated in a high energy collider belongs to.
Data on 78 attributes was collected and collated into a training set of 50,000 points and a validation set of 100,000 points.
Our team's task was to identify a classifier, and its associated hyper-parameters, to maximize classification on the test data with regards to one of four metrics.

This report outlines our approach to this problem, what software we used, our design choices for the code we wrote to accomplish this task, which classifiers we used, and the results of our work.

%TODO: discuss metrics more?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Platform}

This section explains which software libraries and tools we made use of and why we chose them.
See Section \ref{implementation} for how we used them.

\subsection{Project Organization}
We chose to implement this project in Python as we are both more familiar with it and prefer it over MATLAB\textregistered.
For more efficient vector and matrix representations than standard lists, we naturally used the numpy library, as do the two machine learning libraries that we used.

To facilitate team-oriented development, we set up a repository on GitHub to easily merge incremental changes and improvements to our code as well as track task completion.

\subsection{scikit-learn}

%TODO anything to add, @Eugenia?

To learn the data and make predictions we used scikit-learn \cite{pedregosa2011scikit}, one of the more mature Python-based machine learning libraries.
It provides a fairly comprehensive list of classes implementing a common Classifier interface.
This library also provides utility functions for working with data such as shuffling, splitting for cross-validation, and imputing data as well as metrics for scoring classifiers.

\subsection{PyBrain}

Neural networks currently are not fully supported in scikit-learn; so we used PyBrain instead \cite{schaul2010}.
PyBrain allows for programmatically creating arbitrary neural network structures, including multiple layers of hidden nodes.
These networks are trained on numpy arrays of data using various training algorithms provided in the package.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Implementation}
\label{implementation}

This section describes our design choices and reasoning for the software we wrote.

\subsection{Design Overview}

We implemented each classifier as a separate Python script that:
\begin{enumerate}
\item Loads the KDD physics data set
\item Massages it into an appropriate format
\item Trains a Classifier object on the data
\item Prints scores for training and test data

%TODO:cross-validation?

\end{enumerate}

We also created a few utility files for interfacing with some of those from scikit-learn.
These also include tasks such as outputting predictions on the test data in the proper format.

%TODO: @Eugenia, anything to add?

\subsection{Tuning Hyper-Parameters}
\label{tuning_parameters}
%TODO:@Eugenia's approach?
%TODO:tuning for various metrics?

To tune the hyper-parameters of our classifiers, we explored many combinations at once and compared the results from each.
From these, we chose the best parameters and explored slight modifications to them further, iterating this process several times until we saw no change in performance.
The scikit-learn class GridSearchCV helped in this process.
We defined a grid of possible parameters and it ran the given classifier on every combination of them with 3-fold cross-validation.
It also provides an easy method for running multiple jobs in parallel, speeding up the exploration of each parameter space and reducing the time between tuning iterations.

\subsection{Using Multiple Libraries}
\label{multiple_libraries}

We mostly used scikit-learn for learning the data; so our general design approach closely followed that of scikit-learn, including the use of numpy arrays and matrices.
To fit all our Classifiers to a common API, we wrote an adaptor class for a NeuralNetworkClassifier so that we could use PyBrain structures with the scikit-learn Classifier interface.
This made using code applied to Classifiers from both packages easier to share between files and classifiers.
For example, we used the GridSearchCV feature from scikit-learn to quickly explore different parameters for the neural network training algorithm.
This approach worked especially well with proper version controlling as each team member could separately develop new features and then merge them without modifications due to working with different libraries.

%TODO:@Eugenia, too verbose in technical detail here?

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Data Preprocessing}
%TODO
Imputing missing data
removing missing data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Classifiers}

This section describes which classifiers we used as well as what parameters we found to work well and how we found them.
 
%TODO
\subsection{Random Forests}

\subsection{AdaBoost}

\subsection{Gradient Boost}

\subsection{Logistic Regression}

\subsection{Neural Networks}

As described in Section \ref{multiple_libraries}, we wrote an adaptor class for PyBrain so that its neural network classes and functions extend the same interface as the scikit-learn Classifiers.
We used feed-forward neural networks with a back-propagation trainer.
As with the other classifiers, we imputed missing data and employed cross-validation.

The parameters for this model that we explored are the number of hidden nodes (including multiple layers) and also the number of epochs to train the Classifier for.
We primarily explored the various possibilities through the GridSearchCV function mentioned in Section \ref{tuning_parameters}.
Through this mechanism, we discovered that larger numbers of hidden nodes typically must be trained for more epochs.
However, we also found that training for too many epochs tended to overfit the training data, resulting in lower accuracy scores, especially when using cross-validation for testing.

We tried several combinations of parameters for multiple layers of hidden nodes, but did not find any that improved accuracy.
In fact, using multiple layers drastically decreased the accuracy, even when trained for a larger number of epochs.
It is possible that we simply needed to train these networks for still larger numbers of epochs, or with different amounts of hidden nodes at each layer, to realize better scores.
However, the possible combinations of parameters for multiple layers grows much faster than for a single hidden layer as the parameter space becomes a $k$-permutation of the number of nodes for $k$ layers.
Additionally, multiple hidden layer networks require more CPU cycles, and therefore real time, to train for each epoch.
Therefore, we decided to focus on a single hidden layer due to the time constraints of the project.

After identifying a reasonable number of hidden nodes to use in a network and how many epochs to train it for, we explored reducing the number of features.
We first noticed a dramatic improvement when only taking the 20 most informative features and so further explored numbers of features in the range [15,25].
The best results for number of features and model parameters are presented in Section \ref{results}.

Given more time to explore this problem, we could more fully enumerate and test different parameters.
These would include parameters that affect the learning rate and gradient steps of the neural networks.
We found that the defaults for these parameters provided by PyBrain were sufficient for our purposes and varying them would further increase the size of the parameter space to be explored.
When testing different parameters with GridSearchCV, we chose well-spaced values, such as 10, 25, 50, and 100 hidden nodes.
With more time, we could more thoroughly check these ranges to see if some values in between these might actually improve our classifier.

%TODO: quote about how long it takes to run deep belief networks, including with GPUs

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Feature Selection}
%TODO
how we did it
why we did it
  missing data

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Other stuff}
%TODO
2004 winners' report \cite{vogel2004anti}

on ensembling \cite{caruana2004ensemble}

another kdd '04 perspective \cite{caruana2004kdd}

weka approach, which we didn't choose \cite{pfahringer2004weka}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Results}
\label{results}

%TODO
\subsection{Accuracy}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusion}
%TODO

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\bibliographystyle{IEEEtran}
\bibliography{report}

\end{document}
